<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License. See accompanying LICENSE file.
-->

<!-- Put site-specific property overrides in this file. -->

<configuration>
	<property>
            <name>dfs.namenode.name.dir</name>
            <value>/data/hadoop2data/namedata</value>
            <description>Path on the local filesystem where the NameNode stores the namespace and transactions logs persistently.</description>
    </property>
    
    <property>
            <name>dfs.datanode.data.dir</name>
            <value>/data/hadoop2data/data</value>
            <description>Comma separated list ofpaths on the local filesystem of a DataNode where it should store its blocks.</description>
    </property>
    
   	<property>
            <name>dfs.replication</name>
            <value>2</value>
            <description> Default block replication. The actual number of replications can be specified when the file is created.
             	The default is used if replication is not specified in create time.
            </description>
    </property>
    
    <property>
             <name>dfs.blocksize</name>
             <value>512m</value>
             <description> The default block size for new files, in bytes. You can use the following suffix (case insensitive):
             	 k(kilo), m(mega), g(giga), t(tera), p(peta), e(exa) to specify the size (such as 128k, 512m, 1g, etc.), 
             	 Or provide complete size in bytes (such as 134217728 for 128 MB).
              </description>
    </property>
    
    <property>
        <name>dfs.permissions.enabled</name>
        <value>false</value>
    </property>
    
    <property>
      <name>dfs.client.block.write.replace-datanode-on-failure.enable</name>
       <value>false</value>
       <description> If there is a datanode/network failure in the write pipeline, DFSClient will try to remove
        the failed datanode from the pipeline and then continue writing with the remaining datanodes. As a result,
         the number of datanodes in the pipeline is decreased. The feature is to add new datanodes to the pipeline. 
         This is a site-wide property to enable/disable the feature. When the cluster size is extremely small, 
         e.g. 3 nodes or less, cluster administrators may want to set the policy to NEVER in the default configuration file or
          disable this feature. Otherwise, users may experience an unusually high rate of pipeline failures since it is impossible to
           find new datanodes for replacement. See also dfs.client.block.write.replace-datanode-on-failure.policy </description>
    </property>
    
    <property>
        <name>dfs.client.block.write.replace-datanode-on-failure.policy</name>
        <value>NEVER</value>
         <description> This property is used only if the value of dfs.client.block.write.replace-datanode-on-failure.enable is true.
          ALWAYS: always add a new datanode when an existing datanode is removed. 
          NEVER: never add a new datanode. 
          DEFAULT: Let r be the replication number. 
          Let n be the number of existing datanodes. Add a new datanode only if r is greater than or equal to 3 and either (1) floor(r/2) is
           greater than or equal to n; or (2) r is greater than n and the block is hflushed/appended.   </description>
    </property>

    <property>
        <name>dfs.support.append</name>
        <value>true</value>
          <description>Set to true to enable support append for file.</description>
    </property>
    
    <property>
        <name>dfs.namenode.checkpoint.dir</name>
        <value>/data/hadoop2data/secondaryname</value>
        <final>true</final>
        <description> Determines where on the local filesystem the DFS secondary name node should store the temporary images to merge. 
           If this is a comma-delimited list of directories then the image is replicated in all of the directories for redundancy.
        </description>
    </property>
    
    <property>
          <name>dfs.datanode.du.reserved</name>
          <value>10737418240</value>
          <description> Reserved space in bytes per volume. Always leave this much space free for non dfs use.</description>
    </property> 
    
   <!--
    <property>
          <name>dfs.client.read.shortcircuit</name>
          <value>true</value>
          <description>This configuration parameter turns on short-circuit local reads.</description>
    </property>

    <property>
          <name>dfs.client.read.shortcircuit.skip.checksum</name>
          <value>true</value>
          <description>If this configuration parameter is set, short-circuit local reads will skip checksums. 
          This is normally not recommended, but it may be useful for special setups. You might consider using this 
          if you are doing your own checksumming outside of HDFS.</description>
    </property>

    <property>
          <name>dfs.datanode.max.transfer.threads</name>
          <value>4096</value>
          <description>Specifies the maximum number of threads to use for transferring data in and out of the DN.</description>
    </property>  
      
    <property>
        <name>dfs.block.local-path-access.user</name>
        <value>qunzhihe</value>
        <description>Comma separated list of the users allowd to open block files on legacy short-circuit local read.</description>
    </property>
    -->
    
	<property>
    	<name>dfs.datanode.fsdataset.volume.choosing.policy</name>
   	    <value>org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy</value>
   	    <description> AvailableSpaceVolumeChoosingPolicy: A DN volume choosing policy which takes into account the amount of free
  			space on each of the available volumes when considering where to assign a
  			new replica allocation. By default this policy prefers assigning replicas to
  			those volumes with more available free space, so as to over time balance the
  			available space of all the volumes within a DN. 
  	    </description>
    </property>
    
   <property>
     <name>dfs.client.socket-timeout</name>
     <value>60000</value>
     <description>dfs socket timeout</description>
   </property>
    
    <property>
		<name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-threshold</name>
		<value>10737418240</value>
		<description>Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to 
		org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. This setting 
		controls how much DN volumes are allowed to differ in terms of bytes of free disk space before they are considered imbalanced.
		 If the free space of all the volumes are within this range of each other, the volumes will be considered balanced and block assignments
		  will be done on a pure round robin basis. 
		</description>
    </property>
    
    <property>
		<name>dfs.datanode.available-space-volume-choosing-policy.balanced-space-preference-fraction</name>
		<value>0.75</value>
		<description>Only used when the dfs.datanode.fsdataset.volume.choosing.policy is set to 
			org.apache.hadoop.hdfs.server.datanode.fsdataset.AvailableSpaceVolumeChoosingPolicy. 
			This setting controls what percentage of new block allocations will be sent to volumes with more available disk space than others.
			This setting should be in the range 0.0 - 1.0, though in practice 0.5 - 1.0, since there should be no reason to prefer 
			that volumes with less available disk space receive more block allocations.
	    </description>
    </property>
    
    <property>
  		<name>dfs.namenode.fs-limits.min-block-size</name>
  		<value>0</value>
 		 <description> Minimum block size in bytes, enforced by the Namenode at create
     	 	time. This prevents the accidental creation of files with tiny block
      		sizes (and thus many blocks), which can degrade
     		 performance.
     	 </description>
	</property>
	
	<property>
		<name>dfs.namenode.handler.count</name>
		<value>64</value>
		<description>The number of server threads for the namenode.
     	 </description>
	</property>
	
	<property>
		<name>dfs.datanode.handler.count</name>
		<value>64</value>
		<description> The number of server threads for the datanode.
     	 </description>
	</property>
	
	<property>
		<name>dfs.datanode.max.transfer.threads</name>
		<value>8092</value>
		<description> Specifies the maximum number of threads to use for transferring data in and out of the DN.
     	 </description>
	</property>
    
    <property>
		<name>dfs.client.file-block-storage-locations.num-threads</name>
		<value>32</value>
		<description> Number of threads used for making parallel RPCs in DistributedFileSystem#getFileBlockStorageLocations().
     	 </description>
	</property>
	
	<property>
		<name>dfs.qjournal.write-txns.timeout.ms</name>
		<value>100000</value>
		<description> 
     	 </description>
	</property>
	
	
	 <property>
		<name>dfs.namenode.fs-limits.max-directory-items</name>
		<value>6400000</value>
		<description>Defines the maximum number of items that a directory may contain. A value of 0 will disable the check.
     	 </description>
	</property>
    
</configuration>
